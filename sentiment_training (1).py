# -*- coding: utf-8 -*-
"""sentiment_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gtcjeu1aYHUkT6IR2LzJwBGnkh2Yvz1W

# Proyek Analisis Sentimen — Target Bintang 5 (Dicoding)

Notebook ini dibuat agar memenuhi **kriteria utama** dan **saran penilaian bintang 5**:
- Data hasil scraping mandiri (>= 3.000; disarankan >= 10.000)
- Minimal **3 kelas** (negatif/netral/positif)
- **3 eksperimen** dengan minimal **2 kombinasi** berbeda (algoritma / split / fitur)
- Menggunakan **Deep Learning** (Keras + opsi IndoBERT)
- Menyediakan **inference** (output label kategorikal)
- Notebook **harus di-run** sebelum submit (output terlihat)

> Tips bintang 5: pastikan setidaknya 1 eksperimen mendapatkan **akurasi train & test > 92%**.
> Cara paling “aman” biasanya: **fine-tune IndoBERT** + dataset >= 10k + distribusi kelas seimbang.
"""

!pip -q install transformers datasets evaluate accelerate sentencepiece

# Jika di Google Colab (disarankan), jalankan:
# !pip -q install -r requirements.txt

import os, re, math, json, random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

DATA_PATH = "dataset_playstore.csv"  # pastikan file ini hasil scraping mandiri sebelum submit

"""## 1) Load data + cek kolom"""

df = pd.read_csv(DATA_PATH)
print("Rows:", len(df))
print("Columns:", df.columns.tolist())
df.head()

"""## 2) Pelabelan (Kriteria 2)

Jika dataset Anda berasal dari Play Store dan punya kolom `rating`, notebook ini akan:
- membuat/menimpa kolom `label` berdasarkan rating:
  - 1–2 = negatif, 3 = netral, 4–5 = positif
- lalu melakukan validasi distribusi kelas
"""

def rating_to_label(score: int) -> str:
    if score <= 2:
        return "negatif"
    if score == 3:
        return "netral"
    return "positif"

if "rating" in df.columns:
    df["rating"] = df["rating"].fillna(0).astype(int)
    df["label"] = df["rating"].apply(rating_to_label)

# Normalisasi label (jaga-jaga)
df["label"] = df["label"].astype(str).str.lower().str.strip()

# Filter hanya 3 label yang valid
df = df[df["label"].isin(["negatif", "netral", "positif"])].copy()

print("Distribusi label:")
display(df["label"].value_counts())

"""## 3) Data cleaning ringan + balancing (penting untuk akurasi)

Penyebab paling umum akurasi jeblok:
- kelas **netral** terlalu sedikit (imbalance)
- teks terlalu “kotor” (URL/mention/simbol berlebihan)

Di bawah ini ada:
- fungsi cleaning
- opsi balancing sederhana (undersample ke ukuran kelas minimum)
"""

def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", " ", text)
    text = re.sub(r"@\w+", " ", text)
    text = re.sub(r"[^0-9a-zA-Z\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["text"] = df["text"].astype(str)
df["text_clean"] = df["text"].apply(clean_text)

# drop empty
df = df[df["text_clean"].str.len() > 0].copy()

print("Contoh (sebelum/ sesudah):")
display(df[["text", "text_clean", "label"]].sample(5, random_state=SEED))

# OPTIONAL: balancing (disarankan jika distribusi kelas timpang)
vc = df["label"].value_counts()
min_n = int(vc.min())

# gunakan undersample agar seimbang (cepat & stabil)
df_bal = (
    df.groupby("label", group_keys=False)
      .apply(lambda x: x.sample(min_n, random_state=SEED))
      .sample(frac=1.0, random_state=SEED)
      .reset_index(drop=True)
)

print("Sebelum balancing:")
display(vc)
print("\nSesudah balancing:")
display(df_bal["label"].value_counts())

# pilih dataset yang mau dipakai:
df_used = df_bal
print("\nRows used:", len(df_used))

"""## 4) Split data (Kombinasi #1: 80/20)"""

X = df_used["text_clean"].values
y = df_used["label"].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=SEED, stratify=y
)
print("Train:", len(X_train), "Test:", len(X_test))

"""## Eksperimen 1 — TF-IDF + LinearSVC (kelas-weight balanced)"""

tfidf_1 = TfidfVectorizer(
    max_features=50000,
    ngram_range=(1,2),
    min_df=2,
    sublinear_tf=True
)
Xtr1 = tfidf_1.fit_transform(X_train)
Xte1 = tfidf_1.transform(X_test)

svm = LinearSVC(class_weight="balanced")
svm.fit(Xtr1, y_train)

pred_svm = svm.predict(Xte1)
acc_svm = accuracy_score(y_test, pred_svm)

print("Test Accuracy (Exp1 - SVM):", acc_svm)
print(classification_report(y_test, pred_svm, digits=4))

"""## 5) Split data (Kombinasi #2: 70/30) + Eksperimen 2 — TF-IDF + Logistic Regression"""

X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X, y, test_size=0.3, random_state=SEED, stratify=y
)

tfidf_2 = TfidfVectorizer(
    max_features=80000,
    ngram_range=(1,2),
    min_df=2,
    sublinear_tf=True
)
Xtr2 = tfidf_2.fit_transform(X_train2)
Xte2 = tfidf_2.transform(X_test2)

lr = LogisticRegression(
    max_iter=4000,
    class_weight="balanced",
    C=4.0
)
lr.fit(Xtr2, y_train2)

pred_lr = lr.predict(Xte2)
acc_lr = accuracy_score(y_test2, pred_lr)

print("Test Accuracy (Exp2 - LR):", acc_lr)
print(classification_report(y_test2, pred_lr, digits=4))

"""## Eksperimen 3 — Deep Learning (Keras BiGRU)"""

# Encode labels -> int
labels_sorted = ["negatif", "netral", "positif"]
label_to_id = {l:i for i,l in enumerate(labels_sorted)}
id_to_label = {i:l for l,i in label_to_id.items()}

y_train_id = np.array([label_to_id[v] for v in y_train])
y_test_id  = np.array([label_to_id[v] for v in y_test])

# TextVectorization
vocab_size = 60000
seq_len = 80

vectorizer = layers.TextVectorization(
    max_tokens=vocab_size,
    output_mode="int",
    output_sequence_length=seq_len,
)
vectorizer.adapt(X_train)

# Model
inputs = keras.Input(shape=(1,), dtype=tf.string)
x = vectorizer(inputs)
x = layers.Embedding(vocab_size, 128)(x)
x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)
x = layers.GlobalMaxPooling1D()(x)
x = layers.Dropout(0.25)(x)
x = layers.Dense(64, activation="relu")(x)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(3, activation="softmax")(x)

dl = keras.Model(inputs, outputs)
dl.compile(
    optimizer=keras.optimizers.Adam(learning_rate=2e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)
dl.summary()

# Early stopping untuk mencegah overfit
cb = [
    keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=2,
        restore_best_weights=True
    )
]

hist = dl.fit(
    X_train, y_train_id,
    validation_split=0.1,
    epochs=10,
    batch_size=128,
    callbacks=cb,
    verbose=1
)

train_loss, train_acc = dl.evaluate(X_train, y_train_id, verbose=0)
test_loss, test_acc = dl.evaluate(X_test, y_test_id, verbose=0)
print("Train acc:", train_acc)
print("Test  acc:", test_acc)

# Report DL
probs = dl.predict(X_test, verbose=0)
pred_id = probs.argmax(axis=1)
pred_dl = np.array([id_to_label[i] for i in pred_id])

print(classification_report(y_test, pred_dl, digits=4))

"""## (Opsional untuk bintang 5 paling aman) Fine-tune IndoBERT

Jika Anda ingin “mengunci” peluang **>92%**, gunakan IndoBERT (Transformer).
Jalankan cell berikut **di Colab** (butuh internet untuk download model).

Catatan:
- Untuk dataset 10k+ dan label rapi, biasanya IndoBERT bisa tembus 92%+.
- Jika belum tembus, tambahkan data, seimbangkan kelas, dan naikkan epoch 3→5.
"""

# === OPTIONAL INDO-BERT FINE-TUNING (jalankan di Colab) ===
# !pip -q install transformers datasets evaluate accelerate sentencepiece

from datasets import Dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)

# siapkan dataset
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_used["text_clean"].tolist(),
    df_used["label"].map(label_to_id).tolist(),
    test_size=0.2,
    random_state=SEED,
    stratify=df_used["label"].tolist(),
)

ds_train = Dataset.from_dict({"text": train_texts, "label": train_labels})
ds_val   = Dataset.from_dict({"text": val_texts,   "label": val_labels})

model_name = "indobenchmark/indobert-base-p2"  # bisa diganti yg lebih ringan jika perlu
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tok(batch):
    return tokenizer(batch["text"], truncation=True, max_length=512)

ds_train = ds_train.map(tok, batched=True, remove_columns=["text"])
ds_val   = ds_val.map(tok, batched=True, remove_columns=["text"])

collator = DataCollatorWithPadding(tokenizer=tokenizer)

metric_acc = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric_acc.compute(predictions=preds, references=labels)

bert = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,
    id2label=id_to_label,
    label2id=label_to_id,
)

args = TrainingArguments(
    output_dir="bert_out",
    eval_strategy="epoch",          # Changed from evaluation_strategy
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    logging_steps=50,
    report_to="none",
)

trainer = Trainer(
    model=bert,
    args=args,
    train_dataset=ds_train,
    eval_dataset=ds_val,
    tokenizer=tokenizer,
    data_collator=collator,
    compute_metrics=compute_metrics,
)

trainer.train()
res = trainer.evaluate()
print(res)

"""## 6) Ringkasan hasil eksperimen + Confusion Matrix model terbaik"""

# Ambil skor eksperimen yang sudah dijalankan
scores = {
    "Exp1_TFIDF_SVM_80_20": float(acc_svm),
    "Exp2_TFIDF_LR_70_30": float(acc_lr),
    "Exp3_Keras_BiGRU_80_20_test": float(test_acc),
}
scores

best = max(scores, key=scores.get)
print("Best:", best, "score:", scores[best])

if best == "Exp1_TFIDF_SVM_80_20":
    y_true = y_test
    y_pred = pred_svm
    labels = ["negatif","netral","positif"]
elif best == "Exp2_TFIDF_LR_70_30":
    y_true = y_test2
    y_pred = pred_lr
    labels = ["negatif","netral","positif"]
else:
    y_true = y_test
    y_pred = pred_dl
    labels = ["negatif","netral","positif"]

cm = confusion_matrix(y_true, y_pred, labels=labels)

plt.figure(figsize=(6,5))
plt.imshow(cm)
plt.title(f"Confusion Matrix - {best}")
plt.xticks(range(len(labels)), labels, rotation=45)
plt.yticks(range(len(labels)), labels)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha="center", va="center")
plt.tight_layout()
plt.show()

"""## 7) Inference (wajib) — input teks → output label"""

contoh_teks = [
    "Aplikasinya sering error dan bikin emosi",
    "Biasa saja, lumayan buat kebutuhan saya",
    "Mantap! Fiturnya lengkap dan sangat membantu",
    "Netral sih, tapi kadang lemot",
    "Iklan kebanyakan, mengganggu banget"
]

import tensorflow as tf
import numpy as np

def predict_best(texts):
    texts_clean = [clean_text(t) for t in texts]

    if best == "Exp1_TFIDF_SVM_80_20":
        Xv = tfidf_1.transform(texts_clean)
        return svm.predict(Xv)

    if best == "Exp2_TFIDF_LR_70_30":
        Xv = tfidf_2.transform(texts_clean)
        return lr.predict(Xv)

    # Exp3 deep learning (FIX: pakai tf.constant atau list)
    probs = dl.predict(tf.constant(texts_clean, dtype=tf.string), verbose=0)
    ids = probs.argmax(axis=1)
    return np.array([id_to_label[i] for i in ids])

pred = predict_best(contoh_teks)
for t, p in zip(contoh_teks, pred):
    print(f"Teks: {t}\nPrediksi: {p}\n---")